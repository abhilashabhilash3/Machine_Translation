{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kemUcUVphfvr"
      },
      "source": [
        "## Machine Translation\n",
        "A deep neural network that functions as part of an end-to-end machine translation pipeline. The completed pipeline will accept English text as input and return the French translation.\n",
        "\n",
        "- **Preprocess** - Convert text to sequence of integers.\n",
        "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations.\n",
        "- **Prediction** Run the model on English text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEWzfKYfhfvx"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%aimport helper, tests\n",
        "%autoreload 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KCoCB74hfv0",
        "outputId": "428218af-7604-4335-cdb7-87d2a9ee9d35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import helper\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgIy5o0vhfv3",
        "outputId": "8943b2db-03cc-4885-8951-5bb025655e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/cpu:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 14855457735451908914\n",
            ", name: \"/gpu:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 357302272\n",
            "locality {\n",
            "  bus_id: 1\n",
            "}\n",
            "incarnation: 17466780435120687828\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqrbToBhfv4"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNW952E5hfv5",
        "outputId": "a358c504-ca0b-467a-ba83-99acd5aee694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n"
          ]
        }
      ],
      "source": [
        "# Load English data\n",
        "english_sentences = helper.load_data('data/small_vocab_en')\n",
        "# Load French data\n",
        "french_sentences = helper.load_data('data/small_vocab_fr')\n",
        "\n",
        "print('Dataset Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C-cZgcZhfv6",
        "outputId": "1f0c7be4-8525-487e-aa3b-c46c2f6adc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "\n",
            "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "French sample 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "\n",
            "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
            "French sample 3:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "\n",
            "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
            "French sample 4:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "\n",
            "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
            "French sample 5:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sample_i in range(5):\n",
        "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RD-l1Mhfv8"
      },
      "source": [
        "## Preprocess\n",
        "Convert the text into sequences of integers using the following preprocess methods:\n",
        "1. Tokenize the words into ids\n",
        "2. Add padding to make all the sequences the same length.\n",
        "\n",
        "### Tokenize\n",
        "A word level model uses word ids that generate text predictions for each word. They are obtained by turning each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZzfEG2thfv8",
        "outputId": "f4cac616-6255-41e1-c5a3-8222f952ecd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ]
        }
      ],
      "source": [
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\n",
        "\n",
        "tests.test_tokenize(tokenize)\n",
        "\n",
        "# Sample\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPumpqXchfv9"
      },
      "source": [
        "### Padding\n",
        "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, padding is added to the end of the sequences to make them the same length.\n",
        "\n",
        "Hence, all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc0Elt3-hfv9",
        "outputId": "ce696926-4da6-40c6-b1f9-ce82387270bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "def pad(x, length=None):\n",
        "\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n",
        "\n",
        "tests.test_pad(pad)\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JukBua-5hfv-"
      },
      "source": [
        "### Preprocess Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qMZ-djDhfv-",
        "outputId": "0bb9658f-bc03-41a6-e1d4-0c63c0e074e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 199\n",
            "French vocabulary size: 344\n"
          ]
        }
      ],
      "source": [
        "def preprocess(x, y):\n",
        "\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqnFM_Dxhfv_"
      },
      "source": [
        "### Ids Back to Text\n",
        "The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation and helps in better understanding the output of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg8_mqdMhfv_",
        "outputId": "2378e03c-1f8b-4359-a9f4-1c18ea0c5d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ]
        }
      ],
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CIGueLhfwG"
      },
      "source": [
        "### Model\n",
        "A model that incorporates embedding, a bidirectional rnn and an encoder-decoder architecture into one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5kASD_uhfwG",
        "outputId": "a21afcfa-0448-4bd1-cae0-b7a9e0131c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Model Loaded\n"
          ]
        }
      ],
      "source": [
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "    \n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    # Embedding\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
        "                         input_shape=input_shape[1:]))\n",
        "    # Encoder\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tests.test_model_final(model_final)\n",
        "\n",
        "print('Final Model Loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZ_pil7hfwH"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6AF3mGghfwH",
        "outputId": "42b6fd44-0d9b-46bf-f08f-f10a7e7b70b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 15, 128)           25600     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 256)               197376    \n",
            "_________________________________________________________________\n",
            "repeat_vector_4 (RepeatVecto (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 21, 256)           295680    \n",
            "_________________________________________________________________\n",
            "time_distributed_19 (TimeDis (None, 21, 512)           131584    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 21, 512)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_20 (TimeDis (None, 21, 345)           176985    \n",
            "=================================================================\n",
            "Total params: 827,225\n",
            "Trainable params: 827,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/25\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 2.3767 - acc: 0.4869 - val_loss: 1.6101 - val_acc: 0.5805\n",
            "Epoch 2/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 1.4426 - acc: 0.6169 - val_loss: 1.2069 - val_acc: 0.6733\n",
            "Epoch 3/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 1.1652 - acc: 0.6749 - val_loss: 1.0104 - val_acc: 0.7148\n",
            "Epoch 4/25\n",
            "110288/110288 [==============================] - 25s 227us/step - loss: 1.0312 - acc: 0.7028 - val_loss: 0.8671 - val_acc: 0.7403\n",
            "Epoch 5/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.8792 - acc: 0.7353 - val_loss: 0.7295 - val_acc: 0.7727\n",
            "Epoch 6/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.7611 - acc: 0.7647 - val_loss: 0.6033 - val_acc: 0.8118\n",
            "Epoch 7/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.6655 - acc: 0.7905 - val_loss: 0.5105 - val_acc: 0.8414\n",
            "Epoch 8/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.5366 - acc: 0.8301 - val_loss: 0.4155 - val_acc: 0.8703\n",
            "Epoch 9/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.4506 - acc: 0.8574 - val_loss: 0.3298 - val_acc: 0.9017\n",
            "Epoch 10/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.3838 - acc: 0.8802 - val_loss: 0.2805 - val_acc: 0.9198\n",
            "Epoch 11/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.3236 - acc: 0.9008 - val_loss: 0.2133 - val_acc: 0.9422\n",
            "Epoch 12/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.2707 - acc: 0.9185 - val_loss: 0.1839 - val_acc: 0.9496\n",
            "Epoch 13/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.2311 - acc: 0.9311 - val_loss: 0.1610 - val_acc: 0.9554\n",
            "Epoch 14/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.2061 - acc: 0.9389 - val_loss: 0.1486 - val_acc: 0.9587\n",
            "Epoch 15/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1806 - acc: 0.9462 - val_loss: 0.1400 - val_acc: 0.9608\n",
            "Epoch 16/25\n",
            "110288/110288 [==============================] - 25s 228us/step - loss: 0.1653 - acc: 0.9508 - val_loss: 0.1220 - val_acc: 0.9647\n",
            "Epoch 17/25\n",
            "110288/110288 [==============================] - 25s 228us/step - loss: 0.1978 - acc: 0.9393 - val_loss: 0.1268 - val_acc: 0.9633\n",
            "Epoch 18/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1445 - acc: 0.9569 - val_loss: 0.1098 - val_acc: 0.9688\n",
            "Epoch 19/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1284 - acc: 0.9619 - val_loss: 0.1026 - val_acc: 0.9704\n",
            "Epoch 20/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1229 - acc: 0.9631 - val_loss: 0.1036 - val_acc: 0.9701\n",
            "Epoch 21/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1178 - acc: 0.9649 - val_loss: 0.0947 - val_acc: 0.9731\n",
            "Epoch 22/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1059 - acc: 0.9683 - val_loss: 0.0956 - val_acc: 0.9742\n",
            "Epoch 23/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.1007 - acc: 0.9696 - val_loss: 0.0875 - val_acc: 0.9750\n",
            "Epoch 24/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.6314 - acc: 0.8175 - val_loss: 0.3761 - val_acc: 0.8839\n",
            "Epoch 25/25\n",
            "110288/110288 [==============================] - 25s 229us/step - loss: 0.4125 - acc: 0.8758 - val_loss: 0.1613 - val_acc: 0.9634\n",
            "Sample 1:\n",
            "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "Il a vu un vieux camion jaune\n",
            "Sample 2:\n",
            "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ],
      "source": [
        "def final_predictions(x, y, x_tk, y_tk):\n",
        "\n",
        "    model = model_final(x.shape,y.shape[1],\n",
        "                        len(x_tk.word_index)+1,\n",
        "                        len(y_tk.word_index)+1)\n",
        "    model.summary()\n",
        "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
        "\n",
        "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
        "    y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "    sentence = 'he saw a old yellow truck'\n",
        "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
        "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
        "    sentences = np.array([sentence[0], x[0]])\n",
        "    predictions = model.predict(sentences, len(sentences))\n",
        "\n",
        "    print('Sample 1:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "    print('Il a vu un vieux camion jaune')\n",
        "    print('Sample 2:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
        "\n",
        "\n",
        "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "machine_translation.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}